{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 494,
      "metadata": {
        "id": "mwgBR-CZzAMo"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute the gradient using central difference formula\n",
        "def gradient(f, x, h=1e-5):\n",
        "    n = len(x)\n",
        "    grad = np.zeros(n)\n",
        "    for i in range(n):\n",
        "        x_forward = np.copy(x)\n",
        "        x_backward = np.copy(x)\n",
        "        x_forward[i] += h\n",
        "        x_backward[i] -= h\n",
        "        grad[i] = (f(x_forward) - f(x_backward)) / (2 * h)\n",
        "    return grad"
      ],
      "metadata": {
        "id": "ny9geQxrzOFm"
      },
      "execution_count": 495,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hessian(f, x, h=1e-5):\n",
        "    n = len(x)\n",
        "    H = np.zeros((n, n))\n",
        "    for i in range(n):\n",
        "        for j in range(n):\n",
        "            if i == j:\n",
        "                # Diagonal elements: Second partial derivatives w.r.t. the same variable\n",
        "                x_ij_forward = np.copy(x)\n",
        "                x_ij_backward = np.copy(x)\n",
        "                x_ij_forward[i] += h\n",
        "                x_ij_backward[i] -= h\n",
        "                f_ij_forward = f(x_ij_forward)\n",
        "                f_ij_backward = f(x_ij_backward)\n",
        "                H[i, j] = (f_ij_forward - 2*f(x) + f_ij_backward) / (h**2)\n",
        "\n",
        "            else:\n",
        "                # Off-diagonal elements: Mixed partial derivatives\n",
        "               x_ij_forward = np.copy(x)\n",
        "               x_ij_backward = np.copy(x)\n",
        "               x_ij_forward[i] += h\n",
        "               x_ij_forward[j] += h\n",
        "               x_ij_backward[i] -= h\n",
        "               x_ij_backward[j] -= h\n",
        "               f_ij_forward = f(x_ij_forward)\n",
        "               f_ij_backward = f(x_ij_backward)\n",
        "\n",
        "               H[i, j] = (f_ij_forward - f(x_ij_forward - 2*h*np.eye(n)[i]) -\n",
        "                           f(x_ij_forward - 2*h*np.eye(n)[j]) + f_ij_backward) / (4 * h**2)\n",
        "    return H"
      ],
      "metadata": {
        "id": "qcRGY4xPzQKk"
      },
      "execution_count": 496,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#quadratic approximation of model\n",
        "def mk(x,f,p):\n",
        "  grad = gradient(f,x)\n",
        "  hess = hessian(f,x)\n",
        "  return f(x) + np.dot(grad,p) + 0.5*np.dot(p, np.dot(hess, p))"
      ],
      "metadata": {
        "id": "sCtvMvgK0M59"
      },
      "execution_count": 497,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#steepest direction\n",
        "def steep(f,x):\n",
        "  grad = gradient(f,x)\n",
        "  hess = hessian(f,x)\n",
        "  num = np.dot(grad, grad)\n",
        "  denom = np.dot(grad, np.dot(hess, grad))\n",
        "\n",
        "  if denom == 0:\n",
        "    alpha = 1\n",
        "  else:\n",
        "    alpha = num/denom\n",
        "\n",
        "  return -alpha*grad"
      ],
      "metadata": {
        "id": "bxJ0kUNk4Zze"
      },
      "execution_count": 498,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#newton direction\n",
        "def newton(f,x):\n",
        "  grad = gradient(f,x)\n",
        "  hess = hessian(f,x)\n",
        "  hess_inv = np.linalg.inv(hess)\n",
        "  direction = -np.dot(hess_inv,grad)\n",
        "  return direction"
      ],
      "metadata": {
        "id": "0lP-uOkn6EBc"
      },
      "execution_count": 499,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to find theta\n",
        "def theta(steep,newton,delta):\n",
        "  A = np.dot(newton,newton)\n",
        "  B = np.dot(newton,steep)\n",
        "  C = np.dot(steep,steep)\n",
        "\n",
        "  a = A - 2*B + C\n",
        "  b = 2*B - 2*C\n",
        "  c = C - delta**2\n",
        "\n",
        "  if b**2 - 4*a*c < 0:\n",
        "    raise ValueError(\"No real solutions, check inputs\")\n",
        "\n",
        "  num = -b + np.sqrt(b**2 - 4*a*c)\n",
        "  denom = 2*a\n",
        "\n",
        "  return num/denom"
      ],
      "metadata": {
        "id": "HHJsrHGI72d8"
      },
      "execution_count": 500,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def dogleg(f, x0, delta = 1, delta_bar = 5,max_iter = 1000, tol = 1e-4):\n",
        "\n",
        "  x = np.copy(x0)\n",
        "  n = len(x)\n",
        "\n",
        "  for _ in range(max_iter):\n",
        "\n",
        "    g = gradient(f, x)\n",
        "\n",
        "    if np.linalg.norm(g) < tol: return x\n",
        "\n",
        "    p_c = steep(f,x)\n",
        "    p_n = newton(f,x)\n",
        "\n",
        "    if np.linalg.norm(p_c) <= delta:\n",
        "      p = p_n\n",
        "    elif np.linalg.norm(p_c) >= delta:\n",
        "      p = delta*(p_c/np.linalg.norm(p_c))\n",
        "    else:\n",
        "      thetaa = theta(p_c,p_n,delta)\n",
        "      p = thetaa*p_n + (1-thetaa)*p_c\n",
        "\n",
        "    x_new = x + p\n",
        "\n",
        "    rho = (f(x) - f(x_new))/(mk(x,f,p*0) - mk(x,f,p))\n",
        "\n",
        "    if rho > 0.75 and np.linalg.norm(p) == delta:\n",
        "      delta = np.minimum(2*delta,delta_bar)\n",
        "\n",
        "    if rho < 0.25:\n",
        "      delta = 0.25*delta\n",
        "\n",
        "    if rho > 0:\n",
        "      x = x_new"
      ],
      "metadata": {
        "id": "US2wfd0_EVbD"
      },
      "execution_count": 501,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1\n",
        "def f1(x):\n",
        "  return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 -7)**2\n",
        "\n",
        "x = dogleg(f1, [5.0, 2.0])\n",
        "\n",
        "f = f1(x)\n",
        "print(f'Optimal x is {x} and optimal function value is {f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCkcHs6y2z31",
        "outputId": "ecea2616-65a3-4943-c243-c1c3f85761bb"
      },
      "execution_count": 506,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal x is [3.00000026 1.99999993] and optimal function value is 2.2650255576723473e-12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2\n",
        "def f2(x):\n",
        "  return 100*(x[1]-x[0]**2)**2 + (1-x[0])**2\n",
        "\n",
        "x = dogleg(f2, [0.0,3.0])\n",
        "f = f2(x)\n",
        "\n",
        "print(f'Optimal x is {x} and optimal function value is {f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdbR4Bfvp861",
        "outputId": "715db0e8-ed79-4db7-e466-c392a3fbf5ad"
      },
      "execution_count": 507,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal x is [0.99999987 0.99999971] and optimal function value is 6.511664525102119e-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3\n",
        "def f3(x):\n",
        "  return 0.1*(12 + x[0]**2 + ((1 + x[1])**2)/(x[0]**2) + (x[0]**2 + x[1]**2 + 100)/((x[0]*x[1])**4))\n",
        "\n",
        "x = dogleg(f3, [6.0,4.0])\n",
        "f = f3(x)\n",
        "\n",
        "print(f'Optimal x is {x} and optimal function value is {f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OIFqvAxNqSRc",
        "outputId": "7ab20918-1eef-4897-86dc-bd86f72f5a13"
      },
      "execution_count": 504,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal x is [1.90526302 1.82963007] and optimal function value is 1.8560211477891408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4\n",
        "def f4(x):\n",
        "  x1, x2, x3, x4 = x\n",
        "  return (x1 + 10*x2)**2 + 5*(x3-x4)**2 + (x2-2*x3)**4 + 10*(x1-x4)**4\n",
        "\n",
        "x = dogleg(f4, [2,2,1,1])\n",
        "f = f4(x)\n",
        "\n",
        "print(f'Optimal x is {x} and optimal function value is {f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6HtXj5NqXXk",
        "outputId": "dc4cb5cf-5bf7-48ae-f8f3-b203bf06751a"
      },
      "execution_count": 505,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal x is [ 0.01525759 -0.00152576  0.00869266  0.00869266] and optimal function value is 1.464730218158901e-07\n"
          ]
        }
      ]
    }
  ]
}